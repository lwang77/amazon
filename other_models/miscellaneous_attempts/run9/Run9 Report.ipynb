{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 9 is herkeng with batchnorm and dropout in the conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'blooming': 0, 'slash_burn': 1, 'partly_cloudy': 3, 'conventional_mine': 4, 'blow_down': 2, 'haze': 5, 'cloudy': 6, 'road': 7, 'cultivation': 8, 'selective_logging': 9, 'water': 10, 'clear': 11, 'primary': 13, 'agriculture': 14, 'habitation': 12, 'bare_ground': 15, 'artisinal_mine': 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agriculture': 14,\n",
       " 'artisinal_mine': 16,\n",
       " 'bare_ground': 15,\n",
       " 'blooming': 0,\n",
       " 'blow_down': 2,\n",
       " 'clear': 11,\n",
       " 'cloudy': 6,\n",
       " 'conventional_mine': 4,\n",
       " 'cultivation': 8,\n",
       " 'habitation': 12,\n",
       " 'haze': 5,\n",
       " 'partly_cloudy': 3,\n",
       " 'primary': 13,\n",
       " 'road': 7,\n",
       " 'selective_logging': 9,\n",
       " 'slash_burn': 1,\n",
       " 'water': 10}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Epoch 1/20\n",
    "2017-06-18 03:11:33.486050: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
    "2017-06-18 03:11:33.486087: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
    "2017-06-18 03:11:33.486095: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
    "2017-06-18 03:11:33.486102: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
    "2017-06-18 03:11:33.486109: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
    "35000/35000 [==============================] - 2395s - loss: 0.1969 - acc: 0.9229 - val_loss: 0.2757 - val_acc: 0.9066m inf to 0.27571, saving model to weights.hdf5\n",
    "Epoch 2/20\n",
    "34944/35000 [============================>.] - ETA: 3s - loss: 0.1467 - acc: 0.9429 Epoch 00001: val_loss improved from 0.27571 to 0.17968, saving model to weights.35000/35000 [==============================] - 2396s - loss: 0.1467 - acc: 0.9429 - val_loss: 0.1797 - val_acc: 0.9340\n",
    "Epoch 3/20\n",
    "34944/35000 [============================>.] - ETA: 3s - loss: 0.1324 - acc: 0.9488 Epoch 00002: val_loss improved from 0.17968 to 0.13641, saving model to weights.35000/35000 [==============================] - 2395s - loss: 0.1323 - acc: 0.9488 - val_loss: 0.1364 - val_acc: 0.9471\n",
    "Epoch 4/20\n",
    "34944/35000 [============================>.] - ETA: 3s - loss: 0.1247 - acc: 0.9516 Epoch 00003: val_loss improved from 0.13641 to 0.13333, saving model to weights.35000/35000 [==============================] - 2391s - loss: 0.1246 - acc: 0.9516 - val_loss: 0.1333 - val_acc: 0.9487\n",
    "Epoch 5/20\n",
    "34944/35000 [============================>.] - ETA: 3s - loss: 0.1195 - acc: 0.9538 Epoch 00004: val_loss improved from 0.13333 to 0.12759, saving model to weights.35000/35000 [==============================] - 2391s - loss: 0.1195 - acc: 0.9538 - val_loss: 0.1276 - val_acc: 0.9510\n",
    "Epoch 6/20\n",
    "34944/35000 [============================>.] - ETA: 3s - loss: 0.1157 - acc: 0.9553 Epoch 00005: val_loss improved from 0.12759 to 0.12395, saving model to weights.35000/35000 [==============================] - 2399s - loss: 0.1157 - acc: 0.9553 - val_loss: 0.1240 - val_acc: 0.9526\n",
    "Epoch 7/20\n",
    "34944/35000 [============================>.] - ETA: 3s - loss: 0.1127 - acc: 0.9562 Epoch 00006: val_loss improved from 0.12395 to 0.11920, saving model to weights.35000/35000 [==============================] - 2389s - loss: 0.1127 - acc: 0.9562 - val_loss: 0.1192 - val_acc: 0.9533\n",
    "Epoch 8/20\n",
    "35000/35000 [==============================] - 2386s - loss: 0.1101 - acc: 0.9575 - val_loss: 0.1195 - val_acc: 0.9550ove\n",
    "Epoch 9/20\n",
    "35000/35000 [==============================] - 2380s - loss: 0.1087 - acc: 0.9580 - val_loss: 0.1303 - val_acc: 0.9496ove\n",
    "Epoch 10/20\n",
    "34944/35000 [============================>.] - ETA: 3s - loss: 0.1070 - acc: 0.9585 Epoch 00009: val_loss improved from 0.11920 to 0.11405, saving model to weights.35000/35000 [==============================] - 2395s - loss: 0.1070 - acc: 0.9585 - val_loss: 0.1141 - val_acc: 0.9567\n",
    "Epoch 11/20\n",
    "35000/35000 [==============================] - 2385s - loss: 0.1043 - acc: 0.9597 - val_loss: 0.1183 - val_acc: 0.9552ove\n",
    "Epoch 12/20\n",
    "35000/35000 [==============================] - 2390s - loss: 0.1041 - acc: 0.9599 - val_loss: 0.1170 - val_acc: 0.9568ove\n",
    "Epoch 13/20\n",
    "34944/35000 [============================>.] - ETA: 3s - loss: 0.1032 - acc: 0.9603 Epoch 00012: val_loss improved from 0.11405 to 0.11162, saving model to weights.35000/35000 [==============================] - 2380s - loss: 0.1033 - acc: 0.9603 - val_loss: 0.1116 - val_acc: 0.9586\n",
    "Epoch 14/20\n",
    "35000/35000 [==============================] - 2391s - loss: 0.1006 - acc: 0.9610 - val_loss: 0.1136 - val_acc: 0.9574ove\n",
    "Epoch 15/20\n",
    "34944/35000 [============================>.] - ETA: 3s - loss: 0.0999 - acc: 0.9613 Epoch 00014: val_loss improved from 0.11162 to 0.10914, saving model to weights.35000/35000 [==============================] - 2388s - loss: 0.0999 - acc: 0.9613 - val_loss: 0.1091 - val_acc: 0.9586\n",
    "Epoch 16/20\n",
    "35000/35000 [==============================] - 2387s - loss: 0.0993 - acc: 0.9617 - val_loss: 0.1158 - val_acc: 0.9552ove\n",
    "Epoch 17/20\n",
    "35000/35000 [==============================] - 2393s - loss: 0.0983 - acc: 0.9620 - val_loss: 0.1116 - val_acc: 0.9577ove\n",
    "Epoch 18/20\n",
    "34944/35000 [============================>.] - ETA: 3s - loss: 0.0965 - acc: 0.9626 Epoch 00017: val_loss improved from 0.10914 to 0.10490, saving model to weights.35000/35000 [==============================] - 2392s - loss: 0.0965 - acc: 0.9626 - val_loss: 0.1049 - val_acc: 0.9602\n",
    "Epoch 19/20\n",
    "35000/35000 [==============================] - 2394s - loss: 0.0959 - acc: 0.9629 - val_loss: 0.1141 - val_acc: 0.9564ove\n",
    "Epoch 20/20\n",
    "35000/35000 [==============================] - 2382s - loss: 0.0945 - acc: 0.9635 - val_loss: 0.1125 - val_acc: 0.9572ove\n",
    "Train on 35000 samples, validate on 5479 samples\n",
    "Epoch 1/5\n",
    "35000/35000 [==============================] - 2389s - loss: 0.0883 - acc: 0.9658 - val_loss: 0.1064 - val_acc: 0.9606ove\n",
    "Epoch 2/5\n",
    "35000/35000 [==============================] - 2385s - loss: 0.0868 - acc: 0.9662 - val_loss: 0.1050 - val_acc: 0.9609ove\n",
    "Epoch 3/5\n",
    "34944/35000 [============================>.] - ETA: 3s - loss: 0.0858 - acc: 0.9666 Epoch 00002: val_loss improved from 0.10490 to 0.10326, saving model to weights.35000/35000 [==============================] - 2393s - loss: 0.0858 - acc: 0.9666 - val_loss: 0.1033 - val_acc: 0.9617\n",
    "Epoch 4/5\n",
    "35000/35000 [==============================] - 2396s - loss: 0.0849 - acc: 0.9670 - val_loss: 0.1041 - val_acc: 0.9617ove\n",
    "Epoch 5/5\n",
    "35000/35000 [==============================] - 2395s - loss: 0.0844 - acc: 0.9671 - val_loss: 0.1036 - val_acc: 0.9618ove\n",
    "Train on 35000 samples, validate on 5479 samples\n",
    "Epoch 1/5\n",
    "35000/35000 [==============================] - 2399s - loss: 0.0839 - acc: 0.9673 - val_loss: 0.1038 - val_acc: 0.9616ove\n",
    "Epoch 2/5\n",
    "35000/35000 [==============================] - 2395s - loss: 0.0835 - acc: 0.9675 - val_loss: 0.1039 - val_acc: 0.9618ove\n",
    "Epoch 3/5\n",
    "35000/35000 [==============================] - 2399s - loss: 0.0834 - acc: 0.9673 - val_loss: 0.1045 - val_acc: 0.9613ove\n",
    "Epoch 4/5\n",
    "35000/35000 [==============================] - 2385s - loss: 0.0829 - acc: 0.9675 - val_loss: 0.1045 - val_acc: 0.9614ove\n",
    "Epoch 5/5\n",
    "35000/35000 [==============================] - 2384s - loss: 0.0829 - acc: 0.9675 - val_loss: 0.1048 - val_acc: 0.9613ove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max accuracy 0.9618"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "still not as good as Run8 I think"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
